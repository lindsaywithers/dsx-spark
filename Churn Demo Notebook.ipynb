{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Customer Churn with Machine Learning \n",
    "The objective of this notebook is to follow the CRISP-DM methodology to build a model to predict customer churn, and operationalize the model by deploying it into Watson Machine Learning. CRISP-DM stands for cross-industry process for data mining. This methodology provides a structured approach to planning a data mining project.\n",
    "\n",
    "![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n",
    "\n",
    "### Step 1: Load in the data\n",
    "In this section, we will be using our customer data which is being sourced from our S3 connection as well as the churn data which we received as a CSV. Because these data assets have been added to our project, we can easily load them into dataframes with the 'Insert to code' button. Important to note here that we can add data to our project regardless of where it resides, and merge it together for analysis.\n",
    "\n",
    "DSX also provides connector code to load data from and save data to your connected data sources (S3, Apache Hive, IBM Cloudant, IBM DB2, Oracle, Teradata, and more https://datascience.ibm.com/docs/content/analyze-data/python_load.html).\n",
    "\n",
    "Note: You may also want to reference the Spark DataFrame API to learn more about the supported operations, https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ingest.Connectors import Connectors\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "S3loadoptions = {\n",
    "                  Connectors.AmazonS3.ACCESS_KEY          : 'AKIAIYAF6B7L52RTDJPQ',\n",
    "                  Connectors.AmazonS3.SECRET_KEY          : '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',\n",
    "                  Connectors.AmazonS3.SOURCE_BUCKET       : 'demolmw',\n",
    "                  Connectors.AmazonS3.SOURCE_FILE_NAME    : 'customer.csv',\n",
    "                  Connectors.AmazonS3.SOURCE_INFER_SCHEMA : '1',\n",
    "                  Connectors.AmazonS3.SOURCE_FILE_FORMAT  : 'csv'}\n",
    "\n",
    "customer_DF = sqlContext.read.format('com.ibm.spark.discover').options(**S3loadoptions).load()\n",
    "customer_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3loadoptions2 = {\n",
    "                  Connectors.AmazonS3.ACCESS_KEY          : 'AKIAIYAF6B7L52RTDJPQ',\n",
    "                  Connectors.AmazonS3.SECRET_KEY          : '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',\n",
    "                  Connectors.AmazonS3.SOURCE_BUCKET       : 'demolmw',\n",
    "                  Connectors.AmazonS3.SOURCE_FILE_NAME    : 'churn.csv',\n",
    "                  Connectors.AmazonS3.SOURCE_INFER_SCHEMA : '1',\n",
    "                  Connectors.AmazonS3.SOURCE_FILE_FORMAT  : 'csv'}\n",
    "\n",
    "churn_DF = sqlContext.read.format('com.ibm.spark.discover').options(**S3loadoptions2).load()\n",
    "churn_DF.printSchema()\n",
    "churn_DF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Merge Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data=customer_DF.join(churn_DF,customer_DF['ID']==churn_DF['ID']).select(customer_DF['*'],churn_DF['CHURN'])\n",
    "\n",
    "data.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Rename some columns\n",
    "This step is not a requirement, it just makes some column names easier to type with no spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumnRenamed renames an existing column in a Spark DataFrame and returns a new Spark DataFrame\n",
    "\n",
    "data = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\n",
    "data.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = data.toPandas()\n",
    "print \"There are \" + str(len(df_pandas)) + \" observations in the customer history dataset.\"\n",
    "print \"There are \" + str(len(df_pandas.columns)) + \" variables in the dataset.\"\n",
    "\n",
    "print \"\\n******************Descriptive statistics*****************************\\n\"\n",
    "print df_pandas.drop(['ID'], axis = 1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Brunel** Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and more aggressive business users. The system interprets the language and produces visualizations using the user's choice of existing lower-level visualization technologies typically used by application engineers such as RAVE or D3. \n",
    "\n",
    "More information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n",
    "\n",
    "Try Brunel visualization here:  http://brunel.mybluemix.net/gallery_app/renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brunel\n",
    "df_pandas = data.toPandas()\n",
    "%brunel data('df_pandas') stack bar x(Paymethod) y(#count) color(CHURN) bin(Paymethod) percent(#count) label(#count) tooltip(#all) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map\n",
    "%brunel data('df_pandas') x(LocalBilltype) y(Dropped) color(#count:red) style('symbol:rect; size:100%; stroke:none') tooltip(Dropped,#count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PixieDust** is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an interactive UI in which you can explore data.<br/>\n",
    "More information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you haven't already installed it, uncomment and run the following cell to install the pixiedust Python library in your notebook environment. You only need to run it once**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --user --upgrade pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "AVG",
      "chartsize": "55",
      "handlerId": "barChart",
      "keyFields": "Paymethod",
      "rowCount": "500",
      "valueFields": "Usage"
     }
    }
   },
   "outputs": [],
   "source": [
    "from pixiedust.display import *\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build the Spark pipeline and the Random Forest model\n",
    "\"Pipeline\" is an API in SparkML that's used for building models. A pipeline defines a sequence of transformers and estimators to perform tha analysis in stages.<br/>\n",
    "Additional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# StringIndexer encodes a string column of labels to a column of label indices. \n",
    "SI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\n",
    "SI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\n",
    "SI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\n",
    "SI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\n",
    "SI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\n",
    "SI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n",
    "\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n",
    "                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n",
    "                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the label column\n",
    "labelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the pipeline\n",
    "pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "(trainingData, testingData) = data.randomSplit([0.7, 0.3],seed=9)\n",
    "trainingData.cache()\n",
    "testingData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 7: Score the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=model.transform(testingData)\n",
    "result_display=result.select(result[\"ID\"],result[\"CHURN\"],result[\"Label\"],result[\"predictedLabel\"],result[\"prediction\"],result[\"probability\"])\n",
    "result_display.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Model Evaluation\n",
    "Find accuracy of the models and the Area Under the ROC Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Model Accuracy = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 9:  Tune the model to find the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Parameter Grid specifying the parameters to be evaluated to determine the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set different levels for the maxDepth\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder().addGrid(rf.maxDepth,[4,6,8]).build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross validator to tune the pipeline with the generated parameter grid\n",
    "Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 3 fold cross validation\n",
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "# pick the best model\n",
    "best_rfModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the test data set with the best model\n",
    "cvresult=best_rfModel.transform(testingData)\n",
    "cvresults_show=cvresult.select(cvresult[\"ID\"],cvresult[\"CHURN\"],cvresult[\"Label\"],cvresult[\"predictedLabel\"],cvresult[\"prediction\"],cvresult[\"probability\"])\n",
    "cvresults_show.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print 'Model Accuracy of the best fitted model = {:.2f}.'.format(cvresult.filter(cvresult.label == cvresult.prediction).count()/ float(cvresult.count()))\n",
    "print 'Model Accuracy of the default model = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))\n",
    "print '   '\n",
    "print('Area under the ROC curve of best fitted model = {:.2f}.'.format(evaluator.evaluate(cvresult)))\n",
    "print 'Area under the ROC curve of the default model = {:.2f}.'.format(evaluator.evaluate(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Save Model in WML repository\n",
    "\n",
    "In this section you will store your model in the Watson Machine Learning (WML) repository by using Python client libraries.\n",
    "* <a href=\"https://console.ng.bluemix.net/docs/services/PredictiveModeling/index.html\">WML Documentation</a>\n",
    "* <a href=\"http://watson-ml-api.mybluemix.net/\">WML REST API</a> \n",
    "* <a href=\"https://watson-ml-staging-libs.mybluemix.net/repository-python/\">WML Repository API</a>\n",
    "<br/>\n",
    "\n",
    "First, you must import client libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from repository.mlrepositoryclient import MLRepositoryClient\n",
    "from repository.mlrepositoryartifact import MLRepositoryArtifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Action Required</span>\n",
    "\n",
    "If you do not already have an instance of the Machine Learning service in IBM Cloud, go to <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">IBM Cloud</a>, click **Catalog** on the top right of the menu, search for \"Machine Learning\", and create an instance.\n",
    "\n",
    "If you have an existing instance of the Machine Learning service in <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">IBM Cloud</a>, click into the service.\n",
    "\n",
    "* Click **Service credentials** on the left navigation bar\n",
    "* Click **New credentials** and then the **Add** button to create new credentials\n",
    "* Under **ACTIONS** click **View credentials**\n",
    "* Click the **copy** icon to copy the credentials\n",
    "* Paste the credentials into the code cell below\n",
    "\n",
    "![WML Credentials](https://raw.githubusercontent.com/SidneyPhoon/IntroToWMLLab/master/images/WML_Credentials_Jan2018.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Action Required</span>\n",
    "Paste credentials in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "wml_credentials={\n",
    "  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n",
    "  \"access_key\": \"<Insert your WML access key here>\",\n",
    "  \"username\": \"<Insert your WML username here>\",\n",
    "  \"password\": \"<Insert your WML password here>\",\n",
    "  \"instance_id\": \"<Insert your WML instance ID here>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorize the repository client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_repository_client = MLRepositoryClient(wml_credentials.get('url'))\n",
    "ml_repository_client.authorize(wml_credentials.get('username'), wml_credentials.get('password'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model artifact.\n",
    "\n",
    "<b>Tip:</b> The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_artifact = MLRepositoryArtifact(pipeline, name=\"pipelineATF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact = MLRepositoryArtifact(model, training_data=trainingData, name=\"Predict Customer Churn\", pipeline_artifact=pipeline_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model artifact to your Watson Machine Learning instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = ml_repository_client.models.save(model_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the saved model properties\n",
    "print \"modelType: \" + saved_model.meta.prop(\"modelType\")\n",
    "print \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\n",
    "print \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\n",
    "print \"label: \" + saved_model.meta.prop(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Generate the Authorization Token for Invoking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3, requests, json\n",
    "\n",
    "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format(wml_credentials.get('username'), wml_credentials.get('password')))\n",
    "url = '{}/v2/identity/token'.format(wml_credentials.get('url'))\n",
    "response = requests.get(url, headers=headers)\n",
    "mltoken = json.loads(response.text).get('token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12:  Go to WML in IBM Cloud to create a Deployment Endpoint\n",
    "\n",
    "### <span style=\"color:blue\">Action Required</span>\n",
    "\n",
    "* In your <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">IBM Cloud</a> dashboard, click into your WML Service and click the **Launch Dashboard** button under Watson Machine Learing.\n",
    "![WML Launch Dashboard](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/WML_Launch_Dashboard.png)\n",
    "\n",
    "<br/>\n",
    "* You should see your deployed model in the **Models** tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Under *Actions*, click on the 3 ellipses and click ***Create Deployment***.  Give your deployment configuration a unique name, e.g. \"Predict Customer Churn Deply\", select Type=Online and click **Save**.\n",
    "<br/>\n",
    "<br/>\n",
    "* In the *Deployments tab*, under *Actions*, click **View Details**\n",
    "<br/>\n",
    "<br/>\n",
    "* Scoll down to **API Details**, copy the value of the **Scoring Endpoint** into your notepad.  (e.g. \thttps://ibm-watson-ml.mybluemix.net/v2/published_models/64fd0462-3f8a-4b42-820b-59a4da9b7dc6/deployments/7d9995ed-7daf-4cfd-b40f-37cb8ab3d88f/online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13:  Invoke the model through REST API call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a JSON Sample record for the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_payload = {\n",
    "    \"fields\": [\n",
    "    \"ID\",\n",
    "    \"Gender\",\n",
    "    \"Status\",\n",
    "    \"Children\",\n",
    "    \"EstIncome\",\n",
    "    \"CarOwner\",\n",
    "    \"Age\",\n",
    "    \"LongDistance\",\n",
    "    \"International\",\n",
    "    \"Local\",\n",
    "    \"Dropped\",\n",
    "    \"Paymethod\",\n",
    "    \"LocalBilltype\",\n",
    "    \"LongDistanceBilltype\",\n",
    "    \"Usage\",\n",
    "    \"RatePlan\"\n",
    "    ],\n",
    "    \"values\": [ [999,\"F\",\"M\",2.0,77551.100000,\"Y\",33.600000,20.530000,0.000000,41.890000,1.000000,\"CC\",\"Budget\",\"Standard\",62.420000,2.000000] ]\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Rest API call to test the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Action Required</span>\n",
    "Paste your **scoring_endpoint** in the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scoring endpoint from the WML service\n",
    "# Replace the value for scoring_endpoint with your own scoring endpoint\n",
    "scoring_endpoint = '<Insert Scoring Endpoint Here>'\n",
    "header_online = {'Content-Type': 'application/json', 'Authorization': \"Bearer \" + mltoken}\n",
    "\n",
    "# API call here\n",
    "response_scoring = requests.post(scoring_endpoint, json=json_payload, headers=header_online)\n",
    "\n",
    "print response_scoring.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Grab Predicted Value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml = json.loads(response_scoring.text)\n",
    "\n",
    "# First zip the fields and values together\n",
    "zipped_wml = zip(wml['fields'], wml['values'].pop())\n",
    "\n",
    "# Next iterate through items and grab the prediction value\n",
    "print(\"Predicted Churn: \" + [v for (k,v) in zipped_wml if k == 'predictedLabel'].pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You have come to the end of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sidney Phoon**<br/>\n",
    "Jan 3, 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
